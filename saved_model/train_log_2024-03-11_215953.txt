[ Mon Mar 11 22:00:16 2024 ] GIN(
  (gnn): Backbone(
    (convs): ModuleList(
      (0): GINConv(nn=Sequential(
        (0): Linear(in_features=11, out_features=128, bias=True)
        (1): ReLU()
        (2): Linear(in_features=128, out_features=128, bias=True)
      ))
      (1-2): 2 x GINConv(nn=Sequential(
        (0): Linear(in_features=128, out_features=128, bias=True)
        (1): ReLU()
        (2): Linear(in_features=128, out_features=128, bias=True)
      ))
    )
  )
  (projection_head): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (matcher): Mlp(
    (fc1): Linear(in_features=128, out_features=128, bias=True)
    (act): ReLU()
    (fc2): Linear(in_features=128, out_features=11, bias=True)
    (drop): Dropout(p=0.0, inplace=False)
  )
)
[ Mon Mar 11 22:00:16 2024 ] num of parameters: 118682
[ Mon Mar 11 22:00:19 2024 ] epoch: 000/200	data_type: train	batch: 00000/00001	bp loss: 29.13897	
[ Mon Mar 11 22:00:34 2024 ] epoch: 000/200	data_type: train	batch: 00001/00001	bp loss: 237.53148	
[ Mon Mar 11 22:00:34 2024 ] epoch: 000/200	data_type: train	bp loss: 119.7343
[ Mon Mar 11 22:00:39 2024 ] epoch: 000/200	data_type: val  	batch: 51/52	bp loss: 21.8211	
[ Mon Mar 11 22:00:39 2024 ] epoch: 000/200	data_type: val  		bp loss: 196.3357
[ Mon Mar 11 22:00:42 2024 ] data_type: val  		best mean loss: 196.336 (epoch: 000)
[ Mon Mar 11 22:00:43 2024 ] epoch: 001/200	data_type: train	batch: 00000/00001	bp loss: 201.16579	
[ Mon Mar 11 22:00:58 2024 ] epoch: 001/200	data_type: train	batch: 00001/00001	bp loss: 81.94801	
[ Mon Mar 11 22:00:58 2024 ] epoch: 001/200	data_type: train	bp loss: 165.6470
[ Mon Mar 11 22:01:04 2024 ] epoch: 001/200	data_type: val  	batch: 51/52	bp loss: -3.3907	
[ Mon Mar 11 22:01:04 2024 ] epoch: 001/200	data_type: val  		bp loss: -28.7804
[ Mon Mar 11 22:01:07 2024 ] data_type: val  		best mean loss: -28.780 (epoch: 001)
[ Mon Mar 11 22:01:09 2024 ] epoch: 002/200	data_type: train	batch: 00000/00001	bp loss: 5.72060	
[ Mon Mar 11 22:01:24 2024 ] epoch: 002/200	data_type: train	batch: 00001/00001	bp loss: 21.90894	
[ Mon Mar 11 22:01:24 2024 ] epoch: 002/200	data_type: train	bp loss: 12.7234
[ Mon Mar 11 22:01:30 2024 ] epoch: 002/200	data_type: val  	batch: 51/52	bp loss: -2.1218	
[ Mon Mar 11 22:01:30 2024 ] epoch: 002/200	data_type: val  		bp loss: -17.9037
[ Mon Mar 11 22:01:32 2024 ] data_type: val  		best mean loss: -17.904 (epoch: 002)
[ Mon Mar 11 22:01:34 2024 ] epoch: 003/200	data_type: train	batch: 00000/00001	bp loss: 5.08505	
[ Mon Mar 11 22:01:49 2024 ] epoch: 003/200	data_type: train	batch: 00001/00001	bp loss: 21.72650	
[ Mon Mar 11 22:01:49 2024 ] epoch: 003/200	data_type: train	bp loss: 14.4020
[ Mon Mar 11 22:01:55 2024 ] epoch: 003/200	data_type: val  	batch: 51/52	bp loss: 3.4742	
[ Mon Mar 11 22:01:55 2024 ] epoch: 003/200	data_type: val  		bp loss: 25.6791
[ Mon Mar 11 22:01:57 2024 ] data_type: val  		best mean loss: 25.679 (epoch: 003)
[ Mon Mar 11 22:01:59 2024 ] epoch: 004/200	data_type: train	batch: 00000/00001	bp loss: 32.74713	
[ Mon Mar 11 22:02:13 2024 ] epoch: 004/200	data_type: train	batch: 00001/00001	bp loss: 19.86850	
[ Mon Mar 11 22:02:14 2024 ] epoch: 004/200	data_type: train	bp loss: 28.2579
[ Mon Mar 11 22:02:20 2024 ] epoch: 004/200	data_type: val  	batch: 51/52	bp loss: -0.2754	
[ Mon Mar 11 22:02:20 2024 ] epoch: 004/200	data_type: val  		bp loss: -6.7781
[ Mon Mar 11 22:02:22 2024 ] data_type: val  		best mean loss: -6.778 (epoch: 004)
[ Mon Mar 11 22:02:23 2024 ] epoch: 005/200	data_type: train	batch: 00000/00001	bp loss: 0.14013	
[ Mon Mar 11 22:02:37 2024 ] epoch: 005/200	data_type: train	batch: 00001/00001	bp loss: 6.35913	
[ Mon Mar 11 22:02:38 2024 ] epoch: 005/200	data_type: train	bp loss: 3.2524
[ Mon Mar 11 22:02:43 2024 ] epoch: 005/200	data_type: val  	batch: 51/52	bp loss: -0.8917	
[ Mon Mar 11 22:02:43 2024 ] epoch: 005/200	data_type: val  		bp loss: -4.9074
[ Mon Mar 11 22:02:45 2024 ] data_type: val  		best mean loss: -4.907 (epoch: 005)
[ Mon Mar 11 22:02:47 2024 ] epoch: 006/200	data_type: train	batch: 00000/00001	bp loss: 2.52104	
[ Mon Mar 11 22:02:56 2024 ] epoch: 006/200	data_type: train	batch: 00001/00001	bp loss: 13.97442	
[ Mon Mar 11 22:02:57 2024 ] epoch: 006/200	data_type: train	bp loss: 5.9087
[ Mon Mar 11 22:03:02 2024 ] epoch: 006/200	data_type: val  	batch: 51/52	bp loss: 1.2428	
[ Mon Mar 11 22:03:02 2024 ] epoch: 006/200	data_type: val  		bp loss: 13.0283
[ Mon Mar 11 22:03:04 2024 ] data_type: val  		best mean loss: 13.028 (epoch: 006)
[ Mon Mar 11 22:03:05 2024 ] epoch: 007/200	data_type: train	batch: 00000/00001	bp loss: 16.78712	
[ Mon Mar 11 22:03:15 2024 ] epoch: 007/200	data_type: train	batch: 00001/00001	bp loss: 8.24455	
[ Mon Mar 11 22:03:15 2024 ] epoch: 007/200	data_type: train	bp loss: 14.8706
[ Mon Mar 11 22:03:21 2024 ] epoch: 007/200	data_type: val  	batch: 51/52	bp loss: -0.2502	
[ Mon Mar 11 22:03:21 2024 ] epoch: 007/200	data_type: val  		bp loss: -3.6490
[ Mon Mar 11 22:03:23 2024 ] data_type: val  		best mean loss: -3.649 (epoch: 007)
[ Mon Mar 11 22:03:24 2024 ] epoch: 008/200	data_type: train	batch: 00000/00001	bp loss: 1.44820	
[ Mon Mar 11 22:03:34 2024 ] epoch: 008/200	data_type: train	batch: 00001/00001	bp loss: 3.84785	
[ Mon Mar 11 22:03:35 2024 ] epoch: 008/200	data_type: train	bp loss: 2.0464
[ Mon Mar 11 22:03:40 2024 ] epoch: 008/200	data_type: val  	batch: 51/52	bp loss: -0.1049	
[ Mon Mar 11 22:03:40 2024 ] epoch: 008/200	data_type: val  		bp loss: -2.0324
[ Mon Mar 11 22:03:42 2024 ] data_type: val  		best mean loss: -2.032 (epoch: 008)
[ Mon Mar 11 22:03:43 2024 ] epoch: 009/200	data_type: train	batch: 00000/00001	bp loss: 1.52263	
[ Mon Mar 11 22:03:53 2024 ] epoch: 009/200	data_type: train	batch: 00001/00001	bp loss: 0.16184	
[ Mon Mar 11 22:03:54 2024 ] epoch: 009/200	data_type: train	bp loss: 1.5039
[ Mon Mar 11 22:03:58 2024 ] epoch: 009/200	data_type: val  	batch: 51/52	bp loss: 0.0594	
[ Mon Mar 11 22:03:58 2024 ] epoch: 009/200	data_type: val  		bp loss: -0.4547
[ Mon Mar 11 22:04:01 2024 ] data_type: val  		best mean loss: -0.455 (epoch: 009)
[ Mon Mar 11 22:04:02 2024 ] epoch: 010/200	data_type: train	batch: 00000/00001	bp loss: 0.00492	
[ Mon Mar 11 22:04:12 2024 ] epoch: 010/200	data_type: train	batch: 00001/00001	bp loss: 1.45555	
[ Mon Mar 11 22:04:12 2024 ] epoch: 010/200	data_type: train	bp loss: 1.5119
[ Mon Mar 11 22:04:17 2024 ] epoch: 010/200	data_type: val  	batch: 51/52	bp loss: -0.4047	
[ Mon Mar 11 22:04:17 2024 ] epoch: 010/200	data_type: val  		bp loss: 0.3067
[ Mon Mar 11 22:04:19 2024 ] data_type: val  		best mean loss: 0.307 (epoch: 010)
[ Mon Mar 11 22:04:20 2024 ] epoch: 011/200	data_type: train	batch: 00000/00001	bp loss: 3.69526	
[ Mon Mar 11 22:04:30 2024 ] epoch: 011/200	data_type: train	batch: 00001/00001	bp loss: 1.21107	
[ Mon Mar 11 22:04:30 2024 ] epoch: 011/200	data_type: train	bp loss: 1.5760
[ Mon Mar 11 22:04:35 2024 ] epoch: 011/200	data_type: val  	batch: 51/52	bp loss: 0.1436	
[ Mon Mar 11 22:04:35 2024 ] epoch: 011/200	data_type: val  		bp loss: 1.1888
[ Mon Mar 11 22:04:38 2024 ] data_type: val  		best mean loss: 1.189 (epoch: 011)
[ Mon Mar 11 22:04:39 2024 ] epoch: 012/200	data_type: train	batch: 00000/00001	bp loss: 3.92760	
[ Mon Mar 11 22:04:49 2024 ] epoch: 012/200	data_type: train	batch: 00001/00001	bp loss: 0.60179	
[ Mon Mar 11 22:04:49 2024 ] epoch: 012/200	data_type: train	bp loss: 1.8715
[ Mon Mar 11 22:04:54 2024 ] epoch: 012/200	data_type: val  	batch: 51/52	bp loss: -0.0171	
[ Mon Mar 11 22:04:54 2024 ] epoch: 012/200	data_type: val  		bp loss: 1.6883
[ Mon Mar 11 22:04:56 2024 ] data_type: val  		best mean loss: 1.688 (epoch: 012)
[ Mon Mar 11 22:04:58 2024 ] epoch: 013/200	data_type: train	batch: 00000/00001	bp loss: 3.86504	
[ Mon Mar 11 22:05:08 2024 ] epoch: 013/200	data_type: train	batch: 00001/00001	bp loss: 0.93937	
[ Mon Mar 11 22:05:08 2024 ] epoch: 013/200	data_type: train	bp loss: 2.0299
[ Mon Mar 11 22:05:13 2024 ] epoch: 013/200	data_type: val  	batch: 51/52	bp loss: -1.3024	
[ Mon Mar 11 22:05:13 2024 ] epoch: 013/200	data_type: val  		bp loss: -7.7703
[ Mon Mar 11 22:05:15 2024 ] data_type: val  		best mean loss: -7.770 (epoch: 013)
[ Mon Mar 11 22:05:16 2024 ] epoch: 014/200	data_type: train	batch: 00000/00001	bp loss: 7.04183	
[ Mon Mar 11 22:05:26 2024 ] epoch: 014/200	data_type: train	batch: 00001/00001	bp loss: 6.19609	
[ Mon Mar 11 22:05:27 2024 ] epoch: 014/200	data_type: train	bp loss: 7.3145
[ Mon Mar 11 22:05:32 2024 ] epoch: 014/200	data_type: val  	batch: 51/52	bp loss: -1.1089	
[ Mon Mar 11 22:05:32 2024 ] epoch: 014/200	data_type: val  		bp loss: -5.5013
[ Mon Mar 11 22:05:34 2024 ] data_type: val  		best mean loss: -5.501 (epoch: 014)
[ Mon Mar 11 22:05:35 2024 ] epoch: 015/200	data_type: train	batch: 00000/00001	bp loss: 4.12931	
[ Mon Mar 11 22:05:45 2024 ] epoch: 015/200	data_type: train	batch: 00001/00001	bp loss: 2.45765	
[ Mon Mar 11 22:05:45 2024 ] epoch: 015/200	data_type: train	bp loss: 4.0663
[ Mon Mar 11 22:05:50 2024 ] epoch: 015/200	data_type: val  	batch: 51/52	bp loss: 0.4555	
[ Mon Mar 11 22:05:50 2024 ] epoch: 015/200	data_type: val  		bp loss: 4.6907
[ Mon Mar 11 22:05:52 2024 ] data_type: val  		best mean loss: 4.691 (epoch: 015)
[ Mon Mar 11 22:05:54 2024 ] epoch: 016/200	data_type: train	batch: 00000/00001	bp loss: 5.76523	
