[ Mon Mar 11 21:45:14 2024 ] GIN(
  (gnn): Backbone(
    (convs): ModuleList(
      (0): GINConv(nn=Sequential(
        (0): Linear(in_features=11, out_features=128, bias=True)
        (1): ReLU()
        (2): Linear(in_features=128, out_features=128, bias=True)
      ))
      (1-2): 2 x GINConv(nn=Sequential(
        (0): Linear(in_features=128, out_features=128, bias=True)
        (1): ReLU()
        (2): Linear(in_features=128, out_features=128, bias=True)
      ))
    )
  )
  (projection_head): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
  (matcher): Mlp(
    (fc1): Linear(in_features=128, out_features=128, bias=True)
    (act): ReLU()
    (fc2): Linear(in_features=128, out_features=11, bias=True)
    (drop): Dropout(p=0.0, inplace=False)
  )
)
[ Mon Mar 11 21:45:14 2024 ] num of parameters: 118682
[ Mon Mar 11 21:45:17 2024 ] epoch: 000/200	data_type: train	batch: 00000/00001	bp loss: -29.13897	
[ Mon Mar 11 21:45:32 2024 ] epoch: 000/200	data_type: train	batch: 00001/00001	bp loss: -277.09869	
[ Mon Mar 11 21:45:32 2024 ] epoch: 000/200	data_type: train	bp loss: -139.9999
[ Mon Mar 11 21:45:37 2024 ] epoch: 000/200	data_type: val  	batch: 51/52	bp loss: -55.1623	
[ Mon Mar 11 21:45:37 2024 ] epoch: 000/200	data_type: val  		bp loss: -495.9919
[ Mon Mar 11 21:45:40 2024 ] data_type: val  		best mean loss: -495.992 (epoch: 000)
[ Mon Mar 11 21:45:42 2024 ] epoch: 001/200	data_type: train	batch: 00000/00001	bp loss: -476.37787	
[ Mon Mar 11 21:45:57 2024 ] epoch: 001/200	data_type: train	batch: 00001/00001	bp loss: -479.05957	
[ Mon Mar 11 21:45:57 2024 ] epoch: 001/200	data_type: train	bp loss: -515.1908
[ Mon Mar 11 21:46:02 2024 ] epoch: 001/200	data_type: val  	batch: 51/52	bp loss: -67.2676	
[ Mon Mar 11 21:46:02 2024 ] epoch: 001/200	data_type: val  		bp loss: -602.4451
[ Mon Mar 11 21:46:05 2024 ] data_type: val  		best mean loss: -602.445 (epoch: 001)
[ Mon Mar 11 21:46:06 2024 ] epoch: 002/200	data_type: train	batch: 00000/00001	bp loss: -601.61780	
[ Mon Mar 11 21:46:22 2024 ] epoch: 002/200	data_type: train	batch: 00001/00001	bp loss: -518.85992	
[ Mon Mar 11 21:46:22 2024 ] epoch: 002/200	data_type: train	bp loss: -606.3591
[ Mon Mar 11 21:46:27 2024 ] epoch: 002/200	data_type: val  	batch: 51/52	bp loss: -65.1139	
[ Mon Mar 11 21:46:27 2024 ] epoch: 002/200	data_type: val  		bp loss: -614.6561
[ Mon Mar 11 21:46:29 2024 ] data_type: val  		best mean loss: -614.656 (epoch: 002)
[ Mon Mar 11 21:46:31 2024 ] epoch: 003/200	data_type: train	batch: 00000/00001	bp loss: -623.06726	
[ Mon Mar 11 21:46:44 2024 ] epoch: 003/200	data_type: train	batch: 00001/00001	bp loss: -530.25555	
[ Mon Mar 11 21:46:44 2024 ] epoch: 003/200	data_type: train	bp loss: -622.7408
[ Mon Mar 11 21:46:49 2024 ] epoch: 003/200	data_type: val  	batch: 51/52	bp loss: -70.3228	
[ Mon Mar 11 21:46:49 2024 ] epoch: 003/200	data_type: val  		bp loss: -619.3027
[ Mon Mar 11 21:46:51 2024 ] data_type: val  		best mean loss: -619.303 (epoch: 003)
[ Mon Mar 11 21:46:52 2024 ] epoch: 004/200	data_type: train	batch: 00000/00001	bp loss: -629.71222	
[ Mon Mar 11 21:47:07 2024 ] epoch: 004/200	data_type: train	batch: 00001/00001	bp loss: -541.54077	
[ Mon Mar 11 21:47:08 2024 ] epoch: 004/200	data_type: train	bp loss: -630.6988
[ Mon Mar 11 21:47:12 2024 ] epoch: 004/200	data_type: val  	batch: 51/52	bp loss: -67.2226	
[ Mon Mar 11 21:47:12 2024 ] epoch: 004/200	data_type: val  		bp loss: -620.2411
[ Mon Mar 11 21:47:15 2024 ] data_type: val  		best mean loss: -620.241 (epoch: 004)
[ Mon Mar 11 21:47:17 2024 ] epoch: 005/200	data_type: train	batch: 00000/00001	bp loss: -629.84167	
[ Mon Mar 11 21:47:32 2024 ] epoch: 005/200	data_type: train	batch: 00001/00001	bp loss: -528.66309	
[ Mon Mar 11 21:47:32 2024 ] epoch: 005/200	data_type: train	bp loss: -633.5956
[ Mon Mar 11 21:47:36 2024 ] epoch: 005/200	data_type: val  	batch: 51/52	bp loss: -65.4292	
[ Mon Mar 11 21:47:36 2024 ] epoch: 005/200	data_type: val  		bp loss: -622.9746
[ Mon Mar 11 21:47:38 2024 ] data_type: val  		best mean loss: -622.975 (epoch: 005)
[ Mon Mar 11 21:47:40 2024 ] epoch: 006/200	data_type: train	batch: 00000/00001	bp loss: -628.37000	
[ Mon Mar 11 21:47:55 2024 ] epoch: 006/200	data_type: train	batch: 00001/00001	bp loss: -545.62280	
[ Mon Mar 11 21:47:55 2024 ] epoch: 006/200	data_type: train	bp loss: -635.3085
[ Mon Mar 11 21:48:01 2024 ] epoch: 006/200	data_type: val  	batch: 51/52	bp loss: -65.9930	
[ Mon Mar 11 21:48:01 2024 ] epoch: 006/200	data_type: val  		bp loss: -623.2880
[ Mon Mar 11 21:48:04 2024 ] data_type: val  		best mean loss: -623.288 (epoch: 006)
[ Mon Mar 11 21:48:05 2024 ] epoch: 007/200	data_type: train	batch: 00000/00001	bp loss: -638.70319	
[ Mon Mar 11 21:48:20 2024 ] epoch: 007/200	data_type: train	batch: 00001/00001	bp loss: -536.39575	
[ Mon Mar 11 21:48:20 2024 ] epoch: 007/200	data_type: train	bp loss: -637.3448
[ Mon Mar 11 21:48:26 2024 ] epoch: 007/200	data_type: val  	batch: 51/52	bp loss: -65.7547	
[ Mon Mar 11 21:48:26 2024 ] epoch: 007/200	data_type: val  		bp loss: -621.5127
[ Mon Mar 11 21:48:29 2024 ] data_type: val  		best mean loss: -621.513 (epoch: 007)
[ Mon Mar 11 21:48:30 2024 ] epoch: 008/200	data_type: train	batch: 00000/00001	bp loss: -636.97382	
[ Mon Mar 11 21:48:45 2024 ] epoch: 008/200	data_type: train	batch: 00001/00001	bp loss: -532.85742	
[ Mon Mar 11 21:48:45 2024 ] epoch: 008/200	data_type: train	bp loss: -639.6000
[ Mon Mar 11 21:48:50 2024 ] epoch: 008/200	data_type: val  	batch: 51/52	bp loss: -65.7128	
[ Mon Mar 11 21:48:50 2024 ] epoch: 008/200	data_type: val  		bp loss: -620.2826
[ Mon Mar 11 21:48:53 2024 ] data_type: val  		best mean loss: -620.283 (epoch: 008)
[ Mon Mar 11 21:48:54 2024 ] epoch: 009/200	data_type: train	batch: 00000/00001	bp loss: -641.76599	
[ Mon Mar 11 21:49:09 2024 ] epoch: 009/200	data_type: train	batch: 00001/00001	bp loss: -541.79449	
[ Mon Mar 11 21:49:09 2024 ] epoch: 009/200	data_type: train	bp loss: -642.6799
[ Mon Mar 11 21:49:15 2024 ] epoch: 009/200	data_type: val  	batch: 51/52	bp loss: -68.5461	
[ Mon Mar 11 21:49:15 2024 ] epoch: 009/200	data_type: val  		bp loss: -616.8853
[ Mon Mar 11 21:49:17 2024 ] data_type: val  		best mean loss: -616.885 (epoch: 009)
[ Mon Mar 11 21:49:19 2024 ] epoch: 010/200	data_type: train	batch: 00000/00001	bp loss: -636.89062	
[ Mon Mar 11 21:49:34 2024 ] epoch: 010/200	data_type: train	batch: 00001/00001	bp loss: -540.18359	
[ Mon Mar 11 21:49:34 2024 ] epoch: 010/200	data_type: train	bp loss: -646.4817
[ Mon Mar 11 21:49:39 2024 ] epoch: 010/200	data_type: val  	batch: 51/52	bp loss: -61.2918	
[ Mon Mar 11 21:49:39 2024 ] epoch: 010/200	data_type: val  		bp loss: -611.5710
[ Mon Mar 11 21:49:41 2024 ] data_type: val  		best mean loss: -611.571 (epoch: 010)
[ Mon Mar 11 21:49:43 2024 ] epoch: 011/200	data_type: train	batch: 00000/00001	bp loss: -648.27429	
[ Mon Mar 11 21:49:58 2024 ] epoch: 011/200	data_type: train	batch: 00001/00001	bp loss: -552.35968	
[ Mon Mar 11 21:49:58 2024 ] epoch: 011/200	data_type: train	bp loss: -651.3265
[ Mon Mar 11 21:50:04 2024 ] epoch: 011/200	data_type: val  	batch: 51/52	bp loss: -66.3673	
[ Mon Mar 11 21:50:04 2024 ] epoch: 011/200	data_type: val  		bp loss: -599.7686
[ Mon Mar 11 21:50:06 2024 ] data_type: val  		best mean loss: -599.769 (epoch: 011)
[ Mon Mar 11 21:50:08 2024 ] epoch: 012/200	data_type: train	batch: 00000/00001	bp loss: -670.91455	
[ Mon Mar 11 21:50:23 2024 ] epoch: 012/200	data_type: train	batch: 00001/00001	bp loss: -556.35150	
[ Mon Mar 11 21:50:23 2024 ] epoch: 012/200	data_type: train	bp loss: -656.1629
[ Mon Mar 11 21:50:28 2024 ] epoch: 012/200	data_type: val  	batch: 51/52	bp loss: -58.6755	
[ Mon Mar 11 21:50:28 2024 ] epoch: 012/200	data_type: val  		bp loss: -580.3295
[ Mon Mar 11 21:50:31 2024 ] data_type: val  		best mean loss: -580.330 (epoch: 012)
[ Mon Mar 11 21:50:32 2024 ] epoch: 013/200	data_type: train	batch: 00000/00001	bp loss: -657.62848	
[ Mon Mar 11 21:50:48 2024 ] epoch: 013/200	data_type: train	batch: 00001/00001	bp loss: -553.87250	
[ Mon Mar 11 21:50:48 2024 ] epoch: 013/200	data_type: train	bp loss: -659.1906
[ Mon Mar 11 21:50:53 2024 ] epoch: 013/200	data_type: val  	batch: 51/52	bp loss: -62.2433	
[ Mon Mar 11 21:50:53 2024 ] epoch: 013/200	data_type: val  		bp loss: -576.7701
[ Mon Mar 11 21:50:55 2024 ] data_type: val  		best mean loss: -576.770 (epoch: 013)
[ Mon Mar 11 21:50:57 2024 ] epoch: 014/200	data_type: train	batch: 00000/00001	bp loss: -654.61725	
[ Mon Mar 11 21:51:12 2024 ] epoch: 014/200	data_type: train	batch: 00001/00001	bp loss: -562.81482	
[ Mon Mar 11 21:51:12 2024 ] epoch: 014/200	data_type: train	bp loss: -661.8487
[ Mon Mar 11 21:51:17 2024 ] epoch: 014/200	data_type: val  	batch: 51/52	bp loss: -59.4785	
[ Mon Mar 11 21:51:17 2024 ] epoch: 014/200	data_type: val  		bp loss: -574.6380
[ Mon Mar 11 21:51:19 2024 ] data_type: val  		best mean loss: -574.638 (epoch: 014)
[ Mon Mar 11 21:51:21 2024 ] epoch: 015/200	data_type: train	batch: 00000/00001	bp loss: -678.15717	
[ Mon Mar 11 21:51:36 2024 ] epoch: 015/200	data_type: train	batch: 00001/00001	bp loss: -554.32733	
[ Mon Mar 11 21:51:36 2024 ] epoch: 015/200	data_type: train	bp loss: -663.9856
[ Mon Mar 11 21:51:41 2024 ] epoch: 015/200	data_type: val  	batch: 51/52	bp loss: -60.4547	
[ Mon Mar 11 21:51:41 2024 ] epoch: 015/200	data_type: val  		bp loss: -582.4262
[ Mon Mar 11 21:51:43 2024 ] data_type: val  		best mean loss: -582.426 (epoch: 015)
[ Mon Mar 11 21:51:44 2024 ] epoch: 016/200	data_type: train	batch: 00000/00001	bp loss: -672.19891	
[ Mon Mar 11 21:52:00 2024 ] epoch: 016/200	data_type: train	batch: 00001/00001	bp loss: -549.17145	
[ Mon Mar 11 21:52:00 2024 ] epoch: 016/200	data_type: train	bp loss: -665.3491
[ Mon Mar 11 21:52:05 2024 ] epoch: 016/200	data_type: val  	batch: 51/52	bp loss: -63.5500	
[ Mon Mar 11 21:52:05 2024 ] epoch: 016/200	data_type: val  		bp loss: -565.5958
[ Mon Mar 11 21:52:07 2024 ] data_type: val  		best mean loss: -565.596 (epoch: 016)
[ Mon Mar 11 21:52:08 2024 ] epoch: 017/200	data_type: train	batch: 00000/00001	bp loss: -660.35284	
[ Mon Mar 11 21:52:23 2024 ] epoch: 017/200	data_type: train	batch: 00001/00001	bp loss: -567.94513	
[ Mon Mar 11 21:52:24 2024 ] epoch: 017/200	data_type: train	bp loss: -666.5562
[ Mon Mar 11 21:52:28 2024 ] epoch: 017/200	data_type: val  	batch: 51/52	bp loss: -62.1328	
[ Mon Mar 11 21:52:28 2024 ] epoch: 017/200	data_type: val  		bp loss: -593.7006
[ Mon Mar 11 21:52:30 2024 ] data_type: val  		best mean loss: -593.701 (epoch: 017)
[ Mon Mar 11 21:52:31 2024 ] epoch: 018/200	data_type: train	batch: 00000/00001	bp loss: -665.53931	
[ Mon Mar 11 21:52:46 2024 ] epoch: 018/200	data_type: train	batch: 00001/00001	bp loss: -567.85828	
[ Mon Mar 11 21:52:47 2024 ] epoch: 018/200	data_type: train	bp loss: -668.1930
[ Mon Mar 11 21:52:51 2024 ] epoch: 018/200	data_type: val  	batch: 51/52	bp loss: -62.6881	
[ Mon Mar 11 21:52:51 2024 ] epoch: 018/200	data_type: val  		bp loss: -584.7322
[ Mon Mar 11 21:52:53 2024 ] data_type: val  		best mean loss: -584.732 (epoch: 018)
[ Mon Mar 11 21:52:55 2024 ] epoch: 019/200	data_type: train	batch: 00000/00001	bp loss: -672.73810	
[ Mon Mar 11 21:53:10 2024 ] epoch: 019/200	data_type: train	batch: 00001/00001	bp loss: -568.86285	
[ Mon Mar 11 21:53:10 2024 ] epoch: 019/200	data_type: train	bp loss: -669.3276
[ Mon Mar 11 21:53:15 2024 ] epoch: 019/200	data_type: val  	batch: 51/52	bp loss: -65.5150	
[ Mon Mar 11 21:53:15 2024 ] epoch: 019/200	data_type: val  		bp loss: -614.8375
[ Mon Mar 11 21:53:17 2024 ] data_type: val  		best mean loss: -614.837 (epoch: 019)
[ Mon Mar 11 21:53:18 2024 ] epoch: 020/200	data_type: train	batch: 00000/00001	bp loss: -669.52893	
[ Mon Mar 11 21:53:33 2024 ] epoch: 020/200	data_type: train	batch: 00001/00001	bp loss: -567.24329	
[ Mon Mar 11 21:53:34 2024 ] epoch: 020/200	data_type: train	bp loss: -669.9579
[ Mon Mar 11 21:53:38 2024 ] epoch: 020/200	data_type: val  	batch: 51/52	bp loss: -63.6797	
[ Mon Mar 11 21:53:38 2024 ] epoch: 020/200	data_type: val  		bp loss: -609.6144
[ Mon Mar 11 21:53:40 2024 ] data_type: val  		best mean loss: -609.614 (epoch: 020)
[ Mon Mar 11 21:53:41 2024 ] epoch: 021/200	data_type: train	batch: 00000/00001	bp loss: -682.34650	
[ Mon Mar 11 21:53:56 2024 ] epoch: 021/200	data_type: train	batch: 00001/00001	bp loss: -562.94739	
[ Mon Mar 11 21:53:56 2024 ] epoch: 021/200	data_type: train	bp loss: -670.6778
[ Mon Mar 11 21:54:01 2024 ] epoch: 021/200	data_type: val  	batch: 51/52	bp loss: -63.6331	
[ Mon Mar 11 21:54:01 2024 ] epoch: 021/200	data_type: val  		bp loss: -600.0817
[ Mon Mar 11 21:54:03 2024 ] data_type: val  		best mean loss: -600.082 (epoch: 021)
[ Mon Mar 11 21:54:04 2024 ] epoch: 022/200	data_type: train	batch: 00000/00001	bp loss: -665.63568	
[ Mon Mar 11 21:54:19 2024 ] epoch: 022/200	data_type: train	batch: 00001/00001	bp loss: -578.19897	
[ Mon Mar 11 21:54:19 2024 ] epoch: 022/200	data_type: train	bp loss: -670.9629
[ Mon Mar 11 21:54:23 2024 ] epoch: 022/200	data_type: val  	batch: 51/52	bp loss: -61.0599	
[ Mon Mar 11 21:54:23 2024 ] epoch: 022/200	data_type: val  		bp loss: -594.6344
[ Mon Mar 11 21:54:25 2024 ] data_type: val  		best mean loss: -594.634 (epoch: 022)
[ Mon Mar 11 21:54:27 2024 ] epoch: 023/200	data_type: train	batch: 00000/00001	bp loss: -667.11011	
[ Mon Mar 11 21:54:42 2024 ] epoch: 023/200	data_type: train	batch: 00001/00001	bp loss: -563.63202	
[ Mon Mar 11 21:54:42 2024 ] epoch: 023/200	data_type: train	bp loss: -670.9547
[ Mon Mar 11 21:54:46 2024 ] epoch: 023/200	data_type: val  	batch: 51/52	bp loss: -64.5337	
[ Mon Mar 11 21:54:46 2024 ] epoch: 023/200	data_type: val  		bp loss: -598.3472
[ Mon Mar 11 21:54:48 2024 ] data_type: val  		best mean loss: -598.347 (epoch: 023)
[ Mon Mar 11 21:54:49 2024 ] epoch: 024/200	data_type: train	batch: 00000/00001	bp loss: -674.94678	
[ Mon Mar 11 21:55:04 2024 ] epoch: 024/200	data_type: train	batch: 00001/00001	bp loss: -552.19116	
[ Mon Mar 11 21:55:05 2024 ] epoch: 024/200	data_type: train	bp loss: -671.2304
[ Mon Mar 11 21:55:09 2024 ] epoch: 024/200	data_type: val  	batch: 51/52	bp loss: -65.4127	
[ Mon Mar 11 21:55:09 2024 ] epoch: 024/200	data_type: val  		bp loss: -607.2520
[ Mon Mar 11 21:55:11 2024 ] data_type: val  		best mean loss: -607.252 (epoch: 024)
[ Mon Mar 11 21:55:12 2024 ] epoch: 025/200	data_type: train	batch: 00000/00001	bp loss: -683.86975	
[ Mon Mar 11 21:55:27 2024 ] epoch: 025/200	data_type: train	batch: 00001/00001	bp loss: -568.78308	
[ Mon Mar 11 21:55:27 2024 ] epoch: 025/200	data_type: train	bp loss: -671.5196
[ Mon Mar 11 21:55:32 2024 ] epoch: 025/200	data_type: val  	batch: 51/52	bp loss: -67.5252	
[ Mon Mar 11 21:55:32 2024 ] epoch: 025/200	data_type: val  		bp loss: -613.0023
[ Mon Mar 11 21:55:34 2024 ] data_type: val  		best mean loss: -613.002 (epoch: 025)
[ Mon Mar 11 21:55:35 2024 ] epoch: 026/200	data_type: train	batch: 00000/00001	bp loss: -665.47437	
[ Mon Mar 11 21:55:50 2024 ] epoch: 026/200	data_type: train	batch: 00001/00001	bp loss: -572.87518	
[ Mon Mar 11 21:55:50 2024 ] epoch: 026/200	data_type: train	bp loss: -671.6807
[ Mon Mar 11 21:55:55 2024 ] epoch: 026/200	data_type: val  	batch: 51/52	bp loss: -66.9479	
[ Mon Mar 11 21:55:55 2024 ] epoch: 026/200	data_type: val  		bp loss: -613.0561
[ Mon Mar 11 21:55:57 2024 ] data_type: val  		best mean loss: -613.056 (epoch: 026)
[ Mon Mar 11 21:55:58 2024 ] epoch: 027/200	data_type: train	batch: 00000/00001	bp loss: -669.63672	
[ Mon Mar 11 21:56:13 2024 ] epoch: 027/200	data_type: train	batch: 00001/00001	bp loss: -564.17816	
[ Mon Mar 11 21:56:13 2024 ] epoch: 027/200	data_type: train	bp loss: -671.9317
[ Mon Mar 11 21:56:18 2024 ] epoch: 027/200	data_type: val  	batch: 51/52	bp loss: -65.0437	
[ Mon Mar 11 21:56:18 2024 ] epoch: 027/200	data_type: val  		bp loss: -609.8001
[ Mon Mar 11 21:56:20 2024 ] data_type: val  		best mean loss: -609.800 (epoch: 027)
[ Mon Mar 11 21:56:21 2024 ] epoch: 028/200	data_type: train	batch: 00000/00001	bp loss: -678.81396	
[ Mon Mar 11 21:56:36 2024 ] epoch: 028/200	data_type: train	batch: 00001/00001	bp loss: -554.71759	
[ Mon Mar 11 21:56:36 2024 ] epoch: 028/200	data_type: train	bp loss: -672.1967
[ Mon Mar 11 21:56:42 2024 ] epoch: 028/200	data_type: val  	batch: 51/52	bp loss: -68.7279	
[ Mon Mar 11 21:56:42 2024 ] epoch: 028/200	data_type: val  		bp loss: -612.2929
[ Mon Mar 11 21:56:44 2024 ] data_type: val  		best mean loss: -612.293 (epoch: 028)
[ Mon Mar 11 21:56:45 2024 ] epoch: 029/200	data_type: train	batch: 00000/00001	bp loss: -675.73120	
[ Mon Mar 11 21:57:00 2024 ] epoch: 029/200	data_type: train	batch: 00001/00001	bp loss: -556.28564	
[ Mon Mar 11 21:57:00 2024 ] epoch: 029/200	data_type: train	bp loss: -672.4160
[ Mon Mar 11 21:57:05 2024 ] epoch: 029/200	data_type: val  	batch: 51/52	bp loss: -67.4242	
[ Mon Mar 11 21:57:05 2024 ] epoch: 029/200	data_type: val  		bp loss: -619.0796
[ Mon Mar 11 21:57:07 2024 ] data_type: val  		best mean loss: -619.080 (epoch: 029)
[ Mon Mar 11 21:57:08 2024 ] epoch: 030/200	data_type: train	batch: 00000/00001	bp loss: -665.16125	
[ Mon Mar 11 21:57:23 2024 ] epoch: 030/200	data_type: train	batch: 00001/00001	bp loss: -569.63385	
[ Mon Mar 11 21:57:23 2024 ] epoch: 030/200	data_type: train	bp loss: -672.5989
[ Mon Mar 11 21:57:29 2024 ] epoch: 030/200	data_type: val  	batch: 51/52	bp loss: -69.7107	
[ Mon Mar 11 21:57:29 2024 ] epoch: 030/200	data_type: val  		bp loss: -620.8724
[ Mon Mar 11 21:57:31 2024 ] data_type: val  		best mean loss: -620.872 (epoch: 030)
[ Mon Mar 11 21:57:32 2024 ] epoch: 031/200	data_type: train	batch: 00000/00001	bp loss: -682.11169	
[ Mon Mar 11 21:57:47 2024 ] epoch: 031/200	data_type: train	batch: 00001/00001	bp loss: -568.86096	
[ Mon Mar 11 21:57:47 2024 ] epoch: 031/200	data_type: train	bp loss: -672.9110
[ Mon Mar 11 21:57:52 2024 ] epoch: 031/200	data_type: val  	batch: 51/52	bp loss: -64.4265	
[ Mon Mar 11 21:57:52 2024 ] epoch: 031/200	data_type: val  		bp loss: -619.8198
[ Mon Mar 11 21:57:54 2024 ] data_type: val  		best mean loss: -619.820 (epoch: 031)
[ Mon Mar 11 21:57:56 2024 ] epoch: 032/200	data_type: train	batch: 00000/00001	bp loss: -667.45642	
[ Mon Mar 11 21:58:10 2024 ] epoch: 032/200	data_type: train	batch: 00001/00001	bp loss: -554.45276	
[ Mon Mar 11 21:58:11 2024 ] epoch: 032/200	data_type: train	bp loss: -673.1766
[ Mon Mar 11 21:58:16 2024 ] epoch: 032/200	data_type: val  	batch: 51/52	bp loss: -68.1487	
[ Mon Mar 11 21:58:16 2024 ] epoch: 032/200	data_type: val  		bp loss: -622.4514
[ Mon Mar 11 21:58:18 2024 ] data_type: val  		best mean loss: -622.451 (epoch: 032)
[ Mon Mar 11 21:58:19 2024 ] epoch: 033/200	data_type: train	batch: 00000/00001	bp loss: -676.73108	
[ Mon Mar 11 21:58:34 2024 ] epoch: 033/200	data_type: train	batch: 00001/00001	bp loss: -560.33057	
[ Mon Mar 11 21:58:34 2024 ] epoch: 033/200	data_type: train	bp loss: -673.4511
[ Mon Mar 11 21:58:39 2024 ] epoch: 033/200	data_type: val  	batch: 51/52	bp loss: -72.0742	
[ Mon Mar 11 21:58:39 2024 ] epoch: 033/200	data_type: val  		bp loss: -626.7733
[ Mon Mar 11 21:58:42 2024 ] data_type: val  		best mean loss: -626.773 (epoch: 033)
[ Mon Mar 11 21:58:43 2024 ] epoch: 034/200	data_type: train	batch: 00000/00001	bp loss: -662.48846	
[ Mon Mar 11 21:58:58 2024 ] epoch: 034/200	data_type: train	batch: 00001/00001	bp loss: -563.95697	
[ Mon Mar 11 21:58:58 2024 ] epoch: 034/200	data_type: train	bp loss: -673.7590
[ Mon Mar 11 21:59:04 2024 ] epoch: 034/200	data_type: val  	batch: 51/52	bp loss: -67.2264	
[ Mon Mar 11 21:59:04 2024 ] epoch: 034/200	data_type: val  		bp loss: -626.7917
[ Mon Mar 11 21:59:06 2024 ] data_type: val  		best mean loss: -626.792 (epoch: 034)
[ Mon Mar 11 21:59:07 2024 ] epoch: 035/200	data_type: train	batch: 00000/00001	bp loss: -681.75793	
[ Mon Mar 11 21:59:22 2024 ] epoch: 035/200	data_type: train	batch: 00001/00001	bp loss: -568.26239	
[ Mon Mar 11 21:59:22 2024 ] epoch: 035/200	data_type: train	bp loss: -674.0762
[ Mon Mar 11 21:59:28 2024 ] epoch: 035/200	data_type: val  	batch: 51/52	bp loss: -68.5621	
[ Mon Mar 11 21:59:28 2024 ] epoch: 035/200	data_type: val  		bp loss: -628.5916
[ Mon Mar 11 21:59:30 2024 ] data_type: val  		best mean loss: -628.592 (epoch: 035)
[ Mon Mar 11 21:59:31 2024 ] epoch: 036/200	data_type: train	batch: 00000/00001	bp loss: -666.62775	
